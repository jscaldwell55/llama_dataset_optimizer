{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yourusername/llama_dataset_optimizer/blob/main/test_colab_a100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# LLaMA Dataset Optimizer - A100 GPU Testing\n",
    "\n",
    "This notebook tests the LLaMA Dataset Optimizer on Google Colab with A100 GPU.\n",
    "\n",
    "**Requirements:**\n",
    "- Runtime: GPU (A100 High-RAM recommended)\n",
    "- Enable GPU acceleration in Runtime > Change runtime type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-check"
   },
   "source": [
    "## 1. GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"‚úÖ A100 GPU detected!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  GPU is {gpu_name}, not A100\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU available. Please enable GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 2. Repository Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/yourusername/llama_dataset_optimizer.git\n",
    "%cd llama_dataset_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install flash-attention for A100 optimization (optional but recommended)\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-basic"
   },
   "source": [
    "## 3. Basic Functionality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-modules"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/llama_dataset_optimizer')\n",
    "\n",
    "from llama_dataset_optimizer import LlamaDatasetOptimizer\n",
    "import yaml\n",
    "\n",
    "print(\"‚úÖ Successfully imported LlamaDatasetOptimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-config"
   },
   "outputs": [],
   "source": [
    "# Load a configuration\n",
    "with open('configs/llama_3_2_instruct.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "print(f\"Model: {config['model_family']}\")\n",
    "print(f\"Quality filtering batch size: {config['batch_sizes']['quality_filtering']}\")\n",
    "print(f\"Similarity threshold: {config['deduplication']['similarity_threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-sample"
   },
   "source": [
    "## 4. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-sample-data"
   },
   "outputs": [],
   "source": [
    "# Create sample dataset for testing\n",
    "sample_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of France?\",\n",
    "        \"response\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain machine learning in simple terms.\",\n",
    "        \"response\": \"Machine learning is a type of artificial intelligence where computers learn patterns from data to make predictions or decisions without being explicitly programmed for each task.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is 2+2?\",\n",
    "        \"response\": \"2+2 equals 4.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of France?\",  # Duplicate for deduplication test\n",
    "        \"response\": \"Paris is the capital city of France.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created sample dataset with {len(sample_data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-optimizer"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-performance"
   },
   "source": [
    "## 5. Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory-test"
   },
   "outputs": [],
   "source": [
    "# Memory usage test\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "\n",
    "print(f\"Memory usage: {memory_info.rss / 1024**2:.1f} MB\")\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "speed-test"
   },
   "outputs": [],
   "source": [
    "# Speed test with larger dataset\n",
    "import time\n",
    "\n",
    "# Create larger sample dataset\n",
    "large_sample = sample_data * 25  # 100 examples\n",
    "\n",
    "start_time = time.time()\n",
    "large_optimized = optimizer.optimize_dataset(\n",
    "    large_sample,\n",
    "    data_format=DataFormat.INSTRUCTION_RESPONSE\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nLarge dataset optimization:\")\n",
    "print(f\"Dataset size: {len(large_sample)} examples\")\n",
    "print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Speed: {len(large_sample) / (end_time - start_time):.1f} examples/second\")\n",
    "print(f\"Final size: {len(large_optimized)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LLAMA DATASET OPTIMIZER - A100 TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"‚úÖ Model: {config['model_name']}\")\n",
    "print(f\"‚úÖ Processing Speed: {len(large_sample) / (end_time - start_time):.1f} examples/second\")\n",
    "print(f\"‚úÖ Memory Efficient: {torch.cuda.memory_allocated() / 1024**2:.1f} MB GPU memory used\")\n",
    "print(\"\\nThe optimizer is running successfully on A100 GPU! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
