{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yourusername/llama_dataset_optimizer/blob/main/test_colab_a100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# LLaMA Dataset Optimizer - A100 GPU Testing\n",
    "\n",
    "This notebook tests the LLaMA Dataset Optimizer on Google Colab with A100 GPU.\n",
    "\n",
    "**Requirements:**\n",
    "- Runtime: GPU (A100 High-RAM recommended)\n",
    "- Enable GPU acceleration in Runtime > Change runtime type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu-check"
   },
   "source": [
    "## 1. GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"‚úÖ A100 GPU detected!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  GPU is {gpu_name}, not A100\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU available. Please enable GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 2. Repository Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/yourusername/llama_dataset_optimizer.git\n",
    "%cd llama_dataset_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install flash-attention for A100 optimization (optional but recommended)\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-basic"
   },
   "source": [
    "## 3. Basic Functionality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-modules"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/llama_dataset_optimizer')\n",
    "\n",
    "from llama_dataset_optimizer import LlamaDatasetOptimizer\n",
    "import yaml\n",
    "\n",
    "print(\"‚úÖ Successfully imported LlamaDatasetOptimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-config"
   },
   "outputs": [],
   "source": [
    "# Load a configuration\n",
    "with open('configs/llama_3_2_instruct.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "print(f\"Model: {config['model_family']}\")\n",
    "print(f\"Quality filtering batch size: {config['batch_sizes']['quality_filtering']}\")\n",
    "print(f\"Similarity threshold: {config['deduplication']['similarity_threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-sample"
   },
   "source": [
    "## 4. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "create-sample-data"
   },
   "outputs": [],
   "source": "# Create sample dataset for testing\nimport json\n\n# Write sample data to JSONL file\nwith open('sample_data.jsonl', 'w') as f:\n    for item in [\n        {\"instruction\": \"What is the capital of France?\", \"response\": \"The capital of France is Paris.\"},\n        {\"instruction\": \"Explain machine learning in simple terms.\", \"response\": \"Machine learning is a type of artificial intelligence where computers learn patterns from data to make predictions or decisions without being explicitly programmed for each task.\"},\n        {\"instruction\": \"What is 2+2?\", \"response\": \"2+2 equals 4.\"},\n        {\"instruction\": \"What is the capital of France?\", \"response\": \"Paris is the capital city of France.\"}  # Duplicate for deduplication test\n    ]:\n        f.write(json.dumps(item) + '\\n')\n\nprint(\"Created sample_data.jsonl with 4 examples\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "test-optimizer"
   },
   "outputs": [],
   "source": "# Test optimization\noptimizer = LlamaDatasetOptimizer()\n\n# Test with TinyLlama model for fast testing\nprint(\"Starting optimization...\")\noptimized_data = optimizer.optimize(\n    \"sample_data.jsonl\",\n    output_dir=\"output\",\n    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    config=\"llama_3_2_instruct_optimized\",\n    top_k=3,\n    skip_deduplication=False\n)\n\nprint(\"‚úÖ Optimization completed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-performance"
   },
   "source": [
    "## 5. Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory-test"
   },
   "outputs": [],
   "source": [
    "# Memory usage test\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_info = process.memory_info()\n",
    "\n",
    "print(f\"Memory usage: {memory_info.rss / 1024**2:.1f} MB\")\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "speed-test"
   },
   "outputs": [],
   "source": "# Speed test with larger dataset\nimport time\nimport json\n\n# Create larger sample dataset\nprint(\"Creating larger test dataset...\")\nwith open('large_sample_data.jsonl', 'w') as f:\n    for i in range(100):\n        item = {\n            \"instruction\": f\"Question {i}: What is {i} + {i}?\",\n            \"response\": f\"The answer is {i*2}.\"\n        }\n        f.write(json.dumps(item) + '\\n')\n\nstart_time = time.time()\nlarge_optimized = optimizer.optimize(\n    \"large_sample_data.jsonl\",\n    output_dir=\"output_large\",\n    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    config=\"llama_3_2_instruct_optimized\",\n    top_k=50,\n    skip_deduplication=True  # Skip for speed test\n)\nend_time = time.time()\n\nprint(f\"\\nLarge dataset optimization:\")\nprint(f\"Dataset size: 100 examples\")\nprint(f\"Processing time: {end_time - start_time:.2f} seconds\")\nprint(f\"Speed: {100 / (end_time - start_time):.1f} examples/second\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LLAMA DATASET OPTIMIZER - A100 TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"‚úÖ Model: {config['model_name']}\")\n",
    "print(f\"‚úÖ Processing Speed: {len(large_sample) / (end_time - start_time):.1f} examples/second\")\n",
    "print(f\"‚úÖ Memory Efficient: {torch.cuda.memory_allocated() / 1024**2:.1f} MB GPU memory used\")\n",
    "print(\"\\nThe optimizer is running successfully on A100 GPU! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}